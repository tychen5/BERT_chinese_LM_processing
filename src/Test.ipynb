{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenization\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from snownlp import SnowNLP\n",
    "from googletrans import Translator\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import *\n",
    "from summa import keywords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = WS( \"../Model/data\")\n",
    "pos = POS( \"../Model/data\")\n",
    "ner = NER( \"../Model/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "define_dict_path =  \"../data/TextTokenize_zh/company_dict.txt\"\n",
    "news_path =  \"../data/TextTokenize_zh/Foxconn_News_2018.csv\"  # POC_NEWS.csv\n",
    "result_path =  \"../Results/tokenize_Foxconn2018_news_Leo.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_weight = {}\n",
    "with open(define_dict_path, \"r\", encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        key, value = line.split()\n",
    "        word_to_weight[str(key)] = 2\n",
    "dictionary = construct_dictionary(word_to_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(news_path, encoding='cp950')\n",
    "all_news_li = news_df.內容.tolist()\n",
    "all_title_li = news_df.標題.tolist()\n",
    "all_news_li2 = []\n",
    "for title, news in zip(all_title_li, all_news_li):\n",
    "    if type(news) == float:  # news is nan, only title\n",
    "        all_news_li2.append(title)\n",
    "    elif type(title) == float:\n",
    "        all_news_li2.append(news)\n",
    "    else:\n",
    "        all_news_li2.append(title + \"：\" + news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence_list = ws(all_news_li2, recommend_dictionary=dictionary,\n",
    "                        segment_delimiter_set={\",\", \"。\", \":\", \"?\", \"!\", \";\", \"，\", \"：\", \"？\", \"！\", \"；\"})\n",
    "# print(len(word_sentence_list))\n",
    "# print(word_sentence_list)\n",
    "pos_sentence_list = pos(word_sentence_list)\n",
    "entity_sentence_list = ner(word_sentence_list, pos_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['我覺得很怪'],[''],['真的超級奇怪'],['好像有人在騙人']]\n",
    "# sentences = ['好奇怪','到底是要怎樣','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pretrained3.txt\", 'w') as f:\n",
    "    for item in sentences:\n",
    "        f.write(item[0] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env_Leo",
   "language": "python",
   "name": "env_leo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
